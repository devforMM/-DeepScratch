Deep Learning Framework â€œFrom Scratchâ€ â€” From Perceptrons to Transformers, with Zero Black Boxes.

"If you can't implement it, you don't understand it."
â€” Richard Feynman
ğŸ¯ Philosophy

ğŸ§  Understand, Donâ€™t Just Use
True mastery in deep learning comes from building, not just importing.



ğŸ—ï¸ Educational Foundation
An ideal playground for those who want to truly understand how deep learning works under the hood.

ğŸš€ Whatâ€™s Implemented â€” From Scratch
ğŸ§© Core Framework

Custom Layer System with manual forward() and backward() passes

Optimizers: SGD, RMSprop, Adam (manual gradient updates)

Initializers: Xavier, He Normal

Loss Functions: CrossEntropy, MSE, Binary CrossEntropy

Activations: ReLU, Sigmoid, Tanh, Softmax

ğŸ§® All mathematical operations (dot products, convolutions, softmax, etc.) are implemented manually using only low-level PyTorch tensor ops like sum, matmul, log, etc. The only autograd features used are requires_grad and .backward() â€” everything else is 100% handcrafted.

ğŸ§  Architecture Portfolio

MLPs â€” Multi-Layer Perceptrons

CNNs â€” Convolutional Neural Networks

Loop-based (educational) and vectorized (optimized) versions

Custom Conv2D, Pooling, BatchNorm layers

RNNs & LSTMs â€” Recurrent Neural Networks

Transformers â€” Full architecture

Multi-Head Attention

Positional Encoding

Encoder, Decoder, and Full Transformer models

ğŸ§© Model Zoo â€” From Scratch Implementations

MiniGPT â€” Generative Transformer

MiniBERT â€” Bidirectional Encoder

MiniViT â€” Vision Transformer

MiniCLIP â€” Contrastive Languageâ€“Image Model

MiniDETR â€” Detection Transformer

MiniMaskFormer â€” Segmentation Transformer   



ğŸ“¦ DeepScratch/
â”‚
â”œâ”€â”€ ğŸ“ core/ # Core engine (manual forward/backward, optimizers, base model)
â”‚ â”œâ”€â”€ MLP_layer.py # Dense layers & manual linear algebra
â”‚ â”œâ”€â”€ MLP_initializers.py # Xavier, He Normal, Uniform
â”‚ â”œâ”€â”€ optimizers.py # SGD, RMSprop, Adam (from scratch)
â”‚ â”œâ”€â”€ losses.py # CrossEntropy, MSE, BCE
â”‚ â”œâ”€â”€ metrics.py # Accuracy, Precision, Recall, F1
â”‚ â”œâ”€â”€ model_structure.py # Base Model class + training loop
â”‚ â”œâ”€â”€ Dropout_layer.py # Custom dropout
â”‚ â””â”€â”€ *.md # Theoretical explanations
â”‚
â”œâ”€â”€ ğŸ“ CNN/
â”‚ â”œâ”€â”€ ğŸ“ Loop_based_cnn/ # Educational loop-based CNNs
â”‚ â”‚ â”œâ”€â”€ Cnn_layers.py # Manual convolution, pooling, batchnorm
â”‚ â”‚ â”œâ”€â”€ Cnn_operations.py # Pixel-by-pixel conv & backprop
â”‚ â”‚ â””â”€â”€ Cnn_initializers.py # Kernel initialization
â”‚ â”‚
â”‚ â”œâ”€â”€ ğŸ“ Vectorised_Cnn_operations/ # Optimized vectorized CNN version
â”‚ â”‚ â”œâ”€â”€ Vec_cnn_Layers.py
â”‚ â”‚ â””â”€â”€ Vectorised_Cnn_operations.py
â”‚ â”‚
â”‚ â””â”€â”€ resnet.py # Custom handcrafted ResNet
â”‚
â”œâ”€â”€ ğŸ“ Custom_transformers/
â”‚ â”œâ”€â”€ transformeroperations.py # Manual multi-head attention, masking, QKV ops
â”‚ â””â”€â”€ Encoder_Decoders.py # Encoder/Decoder architecture logic
â”‚
â”œâ”€â”€ ğŸ“ Rnn/
â”‚ â”œâ”€â”€ RNN_operations.py # Manual RNN/LSTM ops
â”‚ â”œâ”€â”€ Rnn_Layers.py
â”‚ â”œâ”€â”€ Rnn_model.py
â”‚ â”œâ”€â”€ datasets/ # CSV datasets for translation
â”‚ â””â”€â”€ notebooks/ # Educational comparison vs PyTorch
â”‚
â”œâ”€â”€ ğŸ“ MiniTransformersModels/
â”‚ â”œâ”€â”€ MiniGpt.py
â”‚ â”œâ”€â”€ MiniBert.py
â”‚ â”œâ”€â”€ MiniVit.py
â”‚ â”œâ”€â”€ MiniClip.py
â”‚ â”œâ”€â”€ MiniDetr.py
â”‚ â”œâ”€â”€ MiniSegmentationMaskFormer.py
â”‚ â””â”€â”€ test.ipynb
â”‚
â”œâ”€â”€ ğŸ“ GANs/ # (Planned) Generative Adversarial Networks
â”‚
â”œâ”€â”€ ğŸ“ DeepLearningNotebooks/ # Educational notebooks
â”‚ â”œâ”€â”€ single_perceptron.ipynb
â”‚ â”œâ”€â”€ regression_MLP.ipynb
â”‚ â”œâ”€â”€ Multi_classification_MLP.ipynb
â”‚ â”œâ”€â”€ loop_based_mnist.ipynb
â”‚ â”œâ”€â”€ Vec_Cnn_mnist.ipynb
â”‚ â””â”€â”€ California_housing.ipynb
â”‚
â”œâ”€â”€ ğŸ“ TransformerModelsNotebooks/ # Transformer training notebooks
â”‚ â”œâ”€â”€ MiniGpt_notebook.ipynb
â”‚ â”œâ”€â”€ MiniBert_notebook.ipynb
â”‚ â”œâ”€â”€ MiniClip_Notebook.ipynb
â”‚ â”œâ”€â”€ MiniDetr_notebook.ipynb
â”‚ â”œâ”€â”€ MiniVitClassifier_notebook.ipynb
â”‚ â””â”€â”€ MiniSegTransformer.ipynb
â”‚
â””â”€â”€ ğŸ“ utils/
â”œâ”€â”€ activations.py # ReLU, Sigmoid, Tanh, Softmax (manual)
â”œâ”€â”€ batch_normalization_Layer.py # Custom BatchNorm
â”œâ”€â”€ data_manipulation.py # Mini data loaders
â”œâ”€â”€ learning_rate.py # LR schedulers
â”œâ”€â”€ weight_decay.py # Manual weight decay  
â””â”€â”€ dropout_Layer.py # Custom dropout   

## ğŸ§  Educational Goals
- Learn **how deep learning models really work**  
- Understand **mathematical operations** behind training  
- Write **manual forward and backward passes**  
- Compare handcrafted implementations with PyTorchâ€™s automatic modules  

---


## ğŸ§‘â€ğŸ’» Author
ğŸ“« [abderraoufheboul@gmail.com]  

â­ *If you find this project valuable, give it a star and share it â€” learning deep learning from scratch starts here.*

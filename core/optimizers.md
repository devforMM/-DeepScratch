# Optimizers Module

This module implements several optimization algorithms from scratch in pure PyTorch.  
Each optimizer class includes a custom `update()` method to update model parameters based on their gradients.

## ðŸ“¦ Contents

- `GradientDescent` â€” Standard gradient descent.
- `Momentum` â€” Gradient descent with momentum.
- `Adagrad` â€” Adaptive gradient algorithm.
- `RMSProp` â€” RMSProp optimizer.
- `Adam` â€” Adaptive Moment Estimation optimizer.


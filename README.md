Deep Learning Framework â€œFrom Scratchâ€ â€” From Perceptrons to Transformers, with Zero Black Boxes.

"If you can't implement it, you don't understand it."
â€” Richard Feynman
ğŸ¯ Philosophy

ğŸ§  Understand, Donâ€™t Just Use
True mastery in deep learning comes from building, not just importing.



ğŸ—ï¸ Educational Foundation
An ideal playground for those who want to truly understand how deep learning works under the hood.

ğŸš€ Whatâ€™s Implemented â€” From Scratch
ğŸ§© Core Framework

Custom Layer System with manual forward() and backward() passes

Optimizers: SGD, RMSprop, Adam (manual gradient updates)

Initializers: Xavier, He Normal

Loss Functions: CrossEntropy, MSE, Binary CrossEntropy

Activations: ReLU, Sigmoid, Tanh, Softmax

ğŸ§® All mathematical operations (dot products, convolutions, softmax, etc.) are implemented manually using only low-level PyTorch tensor ops like sum, matmul, log, etc. The only autograd features used are requires_grad and .backward() â€” everything else is 100% handcrafted.

ğŸ§  Architecture Portfolio

MLPs â€” Multi-Layer Perceptrons

CNNs â€” Convolutional Neural Networks

Loop-based (educational) and vectorized (optimized) versions

Custom Conv2D, Pooling, BatchNorm layers

RNNs & LSTMs â€” Recurrent Neural Networks

Transformers â€” Full architecture

Multi-Head Attention

Positional Encoding

Encoder, Decoder, and Full Transformer models

ğŸ§© Model Zoo â€” From Scratch Implementations

MiniGPT â€” Generative Transformer

MiniBERT â€” Bidirectional Encoder

MiniViT â€” Vision Transformer

MiniCLIP â€” Contrastive Languageâ€“Image Model

MiniDETR â€” Detection Transformer

MiniMaskFormer â€” Segmentation Transformer   



ğŸ“¦ DeepScratch/
â”‚
â”œâ”€â”€ ğŸ“ core/                         # Core engine: manual forward/backward passes, optimizers, and model base
â”‚   â”œâ”€â”€ MLp_layer.py                 # Dense layers & initialization (manual linear algebra)
â”‚   â”œâ”€â”€ MLp_initializers.py          # Xavier, He Normal, Uniform
â”‚   â”œâ”€â”€ optimizers.py                # SGD, RMSprop, Adam (from scratch)
â”‚   â”œâ”€â”€ losses.py                    # CrossEntropy, MSE, BCE
â”‚   â”œâ”€â”€ metrics.py                   # Accuracy, Precision, Recall, F1 (custom)
â”‚   â”œâ”€â”€ model_structure.py           # Base Model class handling training loop logic
â”‚   â”œâ”€â”€ Droupout_layer.py            # Custom dropout layer
â”‚   â””â”€â”€ *.md                         # Theory explanations (educational docs)
â”‚
â”œâ”€â”€ ğŸ“ CNN/                          # Custom Convolutional Neural Networks
â”‚   â”œâ”€â”€ ğŸ“ Loop_based_cnn/           # Educational, explicit loop implementations
â”‚   â”‚   â”œâ”€â”€ Cnn_layers.py            # Manual convolution, pooling, batchnorm
â”‚   â”‚   â”œâ”€â”€ Cnn_operations.py        # Pixel-by-pixel conv & backprop
â”‚   â”‚   â””â”€â”€ Cnn_initializers.py      # Kernel initialization logic
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ Vectorised_Cnn_operations/ # Optimized vectorized CNN version
â”‚   â”‚   â”œâ”€â”€ Vec_cnn_Layers.py
â”‚   â”‚   â””â”€â”€ Vectorised_Cnn_operations.py
â”‚   â”‚
â”‚   â””â”€â”€ resnet.py                    # Custom handcrafted ResNet implementation
â”‚
â”œâ”€â”€ ğŸ“ Custom_transformers/           # Low-level transformer mechanics
â”‚   â”œâ”€â”€ transformeroperations.py      # Manual multi-head attention, masking, QKV ops
â”‚   â””â”€â”€ Encoder_Decoders.py           # Encoder/Decoder architecture logic
â”‚
â”œâ”€â”€ ğŸ“ Rnn/                           # Recurrent neural networks (from scratch)
â”‚   â”œâ”€â”€ RNN_oprations.py              # Manual matrix-based RNN/LSTM cell ops
â”‚   â”œâ”€â”€ Rnn_Layers.py                 # Layer abstraction
â”‚   â”œâ”€â”€ Rnn_model.py                  # Full sequence model
â”‚   â”œâ”€â”€ datasets/                     # CSV datasets for multilingual translation
â”‚   â””â”€â”€ notebooks/                    # Educational comparisons vs PyTorch
â”‚
â”œâ”€â”€ ğŸ“ MiniTransformersModels/        # Ready-to-train models built on custom blocks
â”‚   â”œâ”€â”€ MiniGpt.py                    # Generative transformer
â”‚   â”œâ”€â”€ Minibert.py                   # Bidirectional encoder (BERT)
â”‚   â”œâ”€â”€ MiniVit.py                    # Vision Transformer
â”‚   â”œâ”€â”€ MiniClip.py                   # Text-Image contrastive model
â”‚   â”œâ”€â”€ MiniDetr.py                   # Object detection transformer
â”‚   â”œâ”€â”€ MiniSegmeationMaskFormer.py   # Segmentation transformer
â”‚   â””â”€â”€ test.ipynb                    # Validation notebook
â”‚
â”œâ”€â”€ ğŸ“ GANs/                          # Custom generative adversarial networks (planned/under dev)
â”‚
â”œâ”€â”€ ğŸ“ DeepLearningNotebooks/         # Jupyter notebooks for training and demos
â”‚   â”œâ”€â”€ single_perceptron.ipynb       # Manual perceptron implementation
â”‚   â”œâ”€â”€ regression_MLP.ipynb          # Linear regression demo
â”‚   â”œâ”€â”€ Multi_classification_MLP.ipynb
â”‚   â”œâ”€â”€ loop_based_mnist.ipynb        # CNN from scratch
â”‚   â”œâ”€â”€ Vec_Cnn_mnist.ipynb           # Vectorized CNN comparison
â”‚   â””â”€â”€ California_housing.ipynb      # Tabular regression example
â”‚
â”œâ”€â”€ ğŸ“ TranfomerModeslNotebooks/      # Training notebooks for each Transformer variant
â”‚   â”œâ”€â”€ MiniGpt_notebook.ipynb
â”‚   â”œâ”€â”€ MiniBert_notebook.ipynb
â”‚   â”œâ”€â”€ MiniClip_Notebook.ipynb
â”‚   â”œâ”€â”€ MiniDetr_notebook.ipynb
â”‚   â”œâ”€â”€ MiniVitClassifier_notebook.ipynb
â”‚   â””â”€â”€ MiniSegTransformer.ipynb
â”‚
â”œâ”€â”€ ğŸ“ utils/                         # Utility layers and helpers
â”‚   â”œâ”€â”€ activations.py                # ReLU, Sigmoid, Tanh, Softmax (manual)
â”‚   â”œâ”€â”€ batch_normalization_Layer.py  # Custom batchnorm layer
â”‚   â”œâ”€â”€ data_manipulation.py          # Mini data loaders & preprocessing tools
â”‚   â”œâ”€â”€ learning_rate.py              # Dynamic learning rate schedulers
â”‚   â”œâ”€â”€ weight_decay.py               # Manual weight decay implementation
â”‚   â””â”€â”€ dropout_Layer.py 

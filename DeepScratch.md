# ğŸ§  MiniDeep Framework

**MiniDeep** is a lightweight deep learning framework built entirely from scratch using Python.  
It is designed for **learning, experimentation, and full transparency** into how neural networks work internally â€” without relying on high-level libraries like PyTorch or TensorFlow for model logic.

MiniDeep focuses on clarity over performance, making it ideal for:
- Students ğŸ‘©â€ğŸ“ğŸ‘¨â€ğŸ“
- Researchers who want full control ğŸ”¬
- Developers learning deep learning internals âš™ï¸

---

## ğŸ¯ Project Goals

- Understand deep learning **from first principles**
- Implement core neural network components manually
- Keep the codebase **modular, readable, and hackable**
- Avoid â€œblack-boxâ€ abstractions

---

## ğŸ“‚ Project Structure

```text
MiniDeep/
â”œâ”€â”€ core/                    # Core neural network engine
â”‚   â”œâ”€â”€ tensor.py            # Custom Tensor structure & operations
â”‚   â”œâ”€â”€ layer.py             # Base Layer class
â”‚   â”œâ”€â”€ model.py             # Model container
â”‚   â””â”€â”€ loss.py              # Loss functions
â”‚
â”œâ”€â”€ utils/                   # Standalone utilities
â”‚   â”œâ”€â”€ activations.py
â”‚   â”œâ”€â”€ batch_normalization_Layer.py
â”‚   â”œâ”€â”€ droupout_Layer.py
â”‚   â”œâ”€â”€ data_manipulation.py
â”‚   â”œâ”€â”€ learning_rate.py
â”‚   â””â”€â”€ weight_decay.py
â”‚
â”œâ”€â”€ optimizers/              # Optimization algorithms
â”‚   â”œâ”€â”€ sgd.py
â”‚   â”œâ”€â”€ momentum.py
â”‚   â””â”€â”€ adam.py
â”‚
â”œâ”€â”€ examples/                # Training examples & demos
â”‚   â”œâ”€â”€ linear_regression.py
â”‚   â””â”€â”€ classification.py
â”‚
â”œâ”€â”€ tests/                   # Unit tests
â”œâ”€â”€ README.md                # Project documentation
â””â”€â”€ requirements.txt
```

---

## ğŸ§© Core Components

### 1. Tensor Engine
- Custom tensor abstraction
- Manual forward & backward propagation
- Gradient tracking

### 2. Layers
- Dense (Fully Connected)
- Dropout
- Batch Normalization

### 3. Activations
- ReLU / LeakyReLU
- Sigmoid
- Tanh
- Softmax

### 4. Loss Functions
- Mean Squared Error (MSE)
- Binary Cross Entropy
- Categorical Cross Entropy

### 5. Optimizers
- SGD
- Momentum
- Adam

### 6. Training Utilities
- Learning rate schedulers
- Weight decay (L2 regularization)
- Data splitting & cross-validation

---

## ğŸš€ Example Usage

```python
from core.model import Model
from core.layer import Dense
from utils.activations import relu, sigmoid
from optimizers.adam import Adam

model = Model()
model.add(Dense(10, 32, activation=relu))
model.add(Dense(32, 1, activation=sigmoid))

model.compile(
    optimizer=Adam(lr=0.001),
    loss="binary_crossentropy"
)

model.fit(X_train, y_train, epochs=100)
```

---

## ğŸ§ª Educational Focus

MiniDeep is **not** optimized for speed or large-scale production use.

Instead, it prioritizes:
- Readability over performance
- Explicit math over abstractions
- Debuggability and learning

---

## ğŸ“Œ Requirements

- Python 3.9+
- NumPy (optional, depending on modules)

```bash
pip install -r requirements.txt
```

---

## ğŸ› ï¸ Roadmap

- [ ] Convolutional layers (CNN)
- [ ] Recurrent layers (RNN / LSTM)
- [ ] Automatic differentiation engine
- [ ] GPU support (educational)

---

## ğŸ¤ Contributing

Contributions are welcome!  
Feel free to:
- Open issues
- Propose improvements
- Add new layers or utilities

---

## ğŸ“œ License

MIT License â€” free to use, modify, and distribute.

---

Built with â¤ï¸ for learning deep learning from scratch.
